{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import torch as torch\n",
    "from load_data import load_EOD_data\n",
    "from evaluator import evaluate\n",
    "from model import get_loss, StockMixer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456789)\n",
    "torch.random.manual_seed(12345678)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path = '../dataset'\n",
    "market_name = 'NASDAQ'\n",
    "relation_name = 'wikidata'\n",
    "stock_num = 1026\n",
    "lookback_length = 16\n",
    "epochs = 100\n",
    "valid_index = 756\n",
    "test_index = 1008\n",
    "fea_num = 5\n",
    "market_num = 20\n",
    "steps = 1\n",
    "learning_rate = 0.001\n",
    "alpha = 0.1\n",
    "scale_factor = 3\n",
    "activation = 'GELU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../dataset/' + market_name\n",
    "if market_name == \"SP500\":\n",
    "    data = np.load('../dataset/SP500/SP500.npy')\n",
    "    data = data[:, 915:, :]\n",
    "    price_data = data[:, :, -1]\n",
    "    mask_data = np.ones((data.shape[0], data.shape[1]))\n",
    "    eod_data = data\n",
    "    gt_data = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for ticket in range(0, data.shape[0]):\n",
    "        for row in range(1, data.shape[1]):\n",
    "            gt_data[ticket][row] = (data[ticket][row][-1] - data[ticket][row - steps][-1]) / \\\n",
    "                                   data[ticket][row - steps][-1]\n",
    "else:\n",
    "    with open(os.path.join(dataset_path, \"/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/eod_data.pkl\"), \"rb\") as f:\n",
    "        eod_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, \"/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/mask_data.pkl\"), \"rb\") as f:\n",
    "        mask_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, \"/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/gt_data.pkl\"), \"rb\") as f:\n",
    "        gt_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, \"/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/price_data.pkl\"), \"rb\") as f:\n",
    "        price_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_dates = mask_data.shape[1]\n",
    "model = StockMixer(\n",
    "    stocks=stock_num,\n",
    "    time_steps=lookback_length,\n",
    "    channels=fea_num,\n",
    "    market=market_num,\n",
    "    scale=scale_factor\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_valid_loss = np.inf\n",
    "best_valid_perf = None\n",
    "best_test_perf = None\n",
    "batch_offsets = np.arange(start=0, stop=valid_index, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(start_index, end_index):\n",
    "    with torch.no_grad():\n",
    "        cur_valid_pred = np.zeros([stock_num, end_index - start_index], dtype=float)\n",
    "        cur_valid_gt = np.zeros([stock_num, end_index - start_index], dtype=float)\n",
    "        cur_valid_mask = np.zeros([stock_num, end_index - start_index], dtype=float)\n",
    "        loss = 0.\n",
    "        reg_loss = 0.\n",
    "        rank_loss = 0.\n",
    "        for cur_offset in range(start_index - lookback_length - steps + 1, end_index - lookback_length - steps + 1):\n",
    "            data_batch, mask_batch, price_batch, gt_batch = map(\n",
    "\n",
    "                lambda x: torch.Tensor(x).to(device),\n",
    "                get_batch(cur_offset)\n",
    "            )\n",
    "            prediction = model(data_batch)\n",
    "            cur_loss, cur_reg_loss, cur_rank_loss, cur_rr = get_loss(prediction, gt_batch, price_batch, mask_batch,\n",
    "                                                                     stock_num, alpha)\n",
    "            loss += cur_loss.item()\n",
    "            reg_loss += cur_reg_loss.item()\n",
    "            rank_loss += cur_rank_loss.item()\n",
    "            cur_valid_pred[:, cur_offset - (start_index - lookback_length - steps + 1)] = cur_rr[:, 0].cpu()\n",
    "            cur_valid_gt[:, cur_offset - (start_index - lookback_length - steps + 1)] = gt_batch[:, 0].cpu()\n",
    "            cur_valid_mask[:, cur_offset - (start_index - lookback_length - steps + 1)] = mask_batch[:, 0].cpu()\n",
    "        loss = loss / (end_index - start_index)\n",
    "        reg_loss = reg_loss / (end_index - start_index)\n",
    "        rank_loss = rank_loss / (end_index - start_index)\n",
    "        cur_valid_perf = evaluate(cur_valid_pred, cur_valid_gt, cur_valid_mask)\n",
    "    return loss, reg_loss, rank_loss, cur_valid_perf\n",
    "\n",
    "\n",
    "def get_batch(offset=None):\n",
    "    if offset is None:\n",
    "        offset = random.randrange(0, valid_index)\n",
    "    seq_len = lookback_length\n",
    "    mask_batch = mask_data[:, offset: offset + seq_len + steps]\n",
    "    mask_batch = np.min(mask_batch, axis=1)\n",
    "    return (\n",
    "        eod_data[:, offset:offset + seq_len, :],\n",
    "        np.expand_dims(mask_batch, axis=1),\n",
    "        np.expand_dims(price_data[:, offset + seq_len - 1], axis=1),\n",
    "        np.expand_dims(gt_data[:, offset + seq_len + steps - 1], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1##########################################################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m cur_loss, cur_reg_loss, cur_rank_loss, _ \u001b[38;5;241m=\u001b[39m get_loss(prediction, gt_batch, price_batch, mask_batch,\n\u001b[1;32m     15\u001b[0m                                                     stock_num, alpha)\n\u001b[1;32m     16\u001b[0m cur_loss \u001b[38;5;241m=\u001b[39m cur_loss\n\u001b[0;32m---> 17\u001b[0m \u001b[43mcur_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m tra_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cur_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"epoch{}##########################################################\".format(epoch + 1))\n",
    "    np.random.shuffle(batch_offsets)\n",
    "    tra_loss = 0.0\n",
    "    tra_reg_loss = 0.0\n",
    "    tra_rank_loss = 0.0\n",
    "    for j in range(valid_index - lookback_length - steps + 1):\n",
    "        data_batch, mask_batch, price_batch, gt_batch = map(\n",
    "            lambda x: torch.Tensor(x).to(device),\n",
    "            get_batch(batch_offsets[j])\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data_batch)\n",
    "        cur_loss, cur_reg_loss, cur_rank_loss, _ = get_loss(prediction, gt_batch, price_batch, mask_batch,\n",
    "                                                            stock_num, alpha)\n",
    "        cur_loss = cur_loss\n",
    "        cur_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tra_loss += cur_loss.item()\n",
    "        tra_reg_loss += cur_reg_loss.item()\n",
    "        tra_rank_loss += cur_rank_loss.item()\n",
    "    tra_loss = tra_loss / (valid_index - lookback_length - steps + 1)\n",
    "    tra_reg_loss = tra_reg_loss / (valid_index - lookback_length - steps + 1)\n",
    "    tra_rank_loss = tra_rank_loss / (valid_index - lookback_length - steps + 1)\n",
    "    print('Train : loss:{:.2e}  =  {:.2e} + alpha*{:.2e}'.format(tra_loss, tra_reg_loss, tra_rank_loss))\n",
    "\n",
    "    val_loss, val_reg_loss, val_rank_loss, val_perf = validate(valid_index, test_index)\n",
    "    print('Valid : loss:{:.2e}  =  {:.2e} + alpha*{:.2e}'.format(val_loss, val_reg_loss, val_rank_loss))\n",
    "\n",
    "    test_loss, test_reg_loss, test_rank_loss, test_perf = validate(test_index, trade_dates)\n",
    "    print('Test: loss:{:.2e}  =  {:.2e} + alpha*{:.2e}'.format(test_loss, test_reg_loss, test_rank_loss))\n",
    "\n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        best_valid_perf = val_perf\n",
    "        best_test_perf = test_perf\n",
    "\n",
    "    print('Valid performance:\\n', 'mse:{:.2e}, IC:{:.2e}, RIC:{:.2e}, prec@10:{:.2e}, SR:{:.2e}'.format(val_perf['mse'], val_perf['IC'],\n",
    "                                                     val_perf['RIC'], val_perf['prec_10'], val_perf['sharpe5']))\n",
    "    print('Test performance:\\n', 'mse:{:.2e}, IC:{:.2e}, RIC:{:.2e}, prec@10:{:.2e}, SR:{:.2e}'.format(test_perf['mse'], test_perf['IC'],\n",
    "                                                                            test_perf['RIC'], test_perf['prec_10'], test_perf['sharpe5']), '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegressionModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m lookback_length \u001b[38;5;241m*\u001b[39m fea_num\n\u001b[1;32m     52\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 53\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegressionModel\u001b[49m(input_dim, output_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     56\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegressionModel' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Seed initialization\n",
    "np.random.seed(123456789)\n",
    "torch.manual_seed(12345678)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and parameters\n",
    "data_path = '../dataset'\n",
    "market_name = 'SP500'\n",
    "relation_name = 'wikidata'\n",
    "stock_num = 1026\n",
    "lookback_length = 16\n",
    "epochs = 100\n",
    "valid_index = 756\n",
    "test_index = 1008\n",
    "fea_num = 5\n",
    "market_num = 20\n",
    "steps = 1\n",
    "learning_rate = 0.001\n",
    "alpha = 0.1\n",
    "scale_factor = 3\n",
    "\n",
    "dataset_path = os.path.join('../dataset', market_name)\n",
    "if market_name == \"SP500\":\n",
    "    data = np.load(os.path.join(dataset_path, 'SP500.npy'))\n",
    "    data = data[:, 915:, :]\n",
    "    price_data = data[:, :, -1]\n",
    "    mask_data = np.ones((data.shape[0], data.shape[1]))\n",
    "    eod_data = data\n",
    "    gt_data = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for ticket in range(data.shape[0]):\n",
    "        for row in range(1, data.shape[1]):\n",
    "            gt_data[ticket][row] = (data[ticket][row][-1] - data[ticket][row - steps][-1]) / data[ticket][row - steps][-1]\n",
    "else:\n",
    "    with open(os.path.join(dataset_path, 'eod_data.pkl'), 'rb') as f:\n",
    "        eod_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, 'mask_data.pkl'), 'rb') as f:\n",
    "        mask_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, 'gt_data.pkl'), 'rb') as f:\n",
    "        gt_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, 'price_data.pkl'), 'rb') as f:\n",
    "        price_data = pickle.load(f)\n",
    "\n",
    "# Model definition\n",
    "input_dim = lookback_length * fea_num\n",
    "output_dim = 1\n",
    "model = LinearRegressionModel(input_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "batch_offsets = np.arange(0, valid_index, dtype=int)\n",
    "\n",
    "def prepare_input_data(offset, lookback_length, steps):\n",
    "    seq_len = lookback_length\n",
    "    mask_batch = mask_data[:, offset: offset + seq_len + steps]\n",
    "    mask_batch = np.min(mask_batch, axis=1)\n",
    "    return (\n",
    "        eod_data[:, offset:offset + seq_len, :].reshape(-1, input_dim),\n",
    "        np.expand_dims(mask_batch, axis=1),\n",
    "        np.expand_dims(price_data[:, offset + seq_len - 1], axis=1),\n",
    "        np.expand_dims(gt_data[:, offset + seq_len + steps - 1], axis=1)\n",
    "    )\n",
    "\n",
    "def get_batch(offset=None):\n",
    "    if offset is None:\n",
    "        offset = np.random.randint(0, valid_index)\n",
    "    data, mask, price, gt = prepare_input_data(offset, lookback_length, steps)\n",
    "    return torch.tensor(data, dtype=torch.float32).to(device), torch.tensor(mask, dtype=torch.float32).to(device), torch.tensor(price, dtype=torch.float32).to(device), torch.tensor(gt, dtype=torch.float32).to(device)\n",
    "\n",
    "def validate(start_index, end_index):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for offset in range(start_index, end_index):\n",
    "            data, mask, price, gt = get_batch(offset)\n",
    "            prediction = model(data)\n",
    "            loss = criterion(prediction * mask, gt * mask)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / (end_index - start_index)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    np.random.shuffle(batch_offsets)\n",
    "    train_loss = 0.0\n",
    "    for j in range(valid_index - lookback_length - steps):\n",
    "        data, mask, price, gt = get_batch(batch_offsets[j])\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = criterion(prediction * mask, gt * mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= (valid_index - lookback_length - steps)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    val_loss = validate(valid_index, test_index)\n",
    "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    test_loss = validate(test_index, len(mask_data))\n",
    "    print(f'Epoch {epoch + 1}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.0041\n",
      "Valid performance:\n",
      " mse:6.69e-04, IC:-6.34e-03, RIC:3.93e-04, prec@10:6.58e-04, SR:-6.45e-01\n",
      "Test performance:\n",
      " mse:5.70e-04, IC:-4.54e-03, RIC:5.10e-03, prec@10:3.06e-04, SR:-6.88e-01 \n",
      "\n",
      "\n",
      "Epoch 2, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.81e-04, IC:-3.18e-03, RIC:-2.13e-03, prec@10:7.88e-04, SR:-4.66e-01\n",
      "Test performance:\n",
      " mse:4.80e-04, IC:-2.37e-03, RIC:-5.96e-03, prec@10:1.57e-04, SR:-4.92e-01 \n",
      "\n",
      "\n",
      "Epoch 3, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.06e-04, IC:5.23e-03, RIC:4.61e-03, prec@10:1.21e-03, SR:-2.00e-01\n",
      "Test performance:\n",
      " mse:4.05e-04, IC:3.78e-03, RIC:-1.26e-02, prec@10:2.80e-04, SR:-2.00e-01 \n",
      "\n",
      "\n",
      "Epoch 4, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.99e-04, IC:3.07e-02, RIC:-7.22e-04, prec@10:1.96e-03, SR:5.74e-01\n",
      "Test performance:\n",
      " mse:5.58e-04, IC:3.00e-02, RIC:-2.01e-02, prec@10:4.18e-03, SR:6.76e-01 \n",
      "\n",
      "\n",
      "Epoch 5, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.44e-04, IC:3.07e-02, RIC:-2.20e-03, prec@10:1.90e-03, SR:4.38e-01\n",
      "Test performance:\n",
      " mse:4.84e-04, IC:2.84e-02, RIC:1.01e-02, prec@10:3.30e-03, SR:5.19e-01 \n",
      "\n",
      "\n",
      "Epoch 6, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.76e-04, IC:1.08e-02, RIC:-4.97e-04, prec@10:1.22e-03, SR:-5.80e-02\n",
      "Test performance:\n",
      " mse:3.82e-04, IC:6.04e-03, RIC:1.32e-03, prec@10:2.84e-03, SR:-5.99e-02 \n",
      "\n",
      "\n",
      "Epoch 7, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:6.31e-04, IC:3.27e-02, RIC:-3.17e-03, prec@10:2.54e-03, SR:6.82e-01\n",
      "Test performance:\n",
      " mse:5.92e-04, IC:3.17e-02, RIC:8.30e-03, prec@10:3.59e-03, SR:7.72e-01 \n",
      "\n",
      "\n",
      "Epoch 8, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.77e-04, IC:1.31e-02, RIC:-6.91e-04, prec@10:1.54e-03, SR:-1.29e-01\n",
      "Test performance:\n",
      " mse:3.83e-04, IC:7.22e-03, RIC:7.67e-03, prec@10:2.20e-03, SR:-1.36e-01 \n",
      "\n",
      "\n",
      "Epoch 9, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.21e-04, IC:-5.94e-03, RIC:-1.03e-03, prec@10:8.90e-04, SR:-3.78e-01\n",
      "Test performance:\n",
      " mse:4.36e-04, IC:-8.98e-03, RIC:2.76e-03, prec@10:2.79e-03, SR:-4.30e-01 \n",
      "\n",
      "\n",
      "Epoch 10, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.99e-04, IC:-2.58e-03, RIC:2.57e-03, prec@10:1.10e-03, SR:-2.88e-01\n",
      "Test performance:\n",
      " mse:4.08e-04, IC:-3.13e-03, RIC:-1.96e-03, prec@10:2.94e-03, SR:-3.24e-01 \n",
      "\n",
      "\n",
      "Epoch 11, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.35e-04, IC:-2.09e-02, RIC:-1.63e-03, prec@10:5.23e-04, SR:-6.75e-01\n",
      "Test performance:\n",
      " mse:5.76e-04, IC:-1.82e-02, RIC:1.44e-03, prec@10:2.17e-03, SR:-7.73e-01 \n",
      "\n",
      "\n",
      "Epoch 12, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:7.86e-04, IC:3.62e-02, RIC:9.78e-04, prec@10:2.53e-03, SR:9.35e-01\n",
      "Test performance:\n",
      " mse:8.00e-04, IC:3.65e-02, RIC:-8.35e-03, prec@10:1.40e-03, SR:1.07e+00 \n",
      "\n",
      "\n",
      "Epoch 13, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.84e-04, IC:2.51e-02, RIC:-2.25e-03, prec@10:2.13e-03, SR:2.24e-01\n",
      "Test performance:\n",
      " mse:3.98e-04, IC:2.41e-02, RIC:-7.06e-03, prec@10:3.08e-03, SR:2.50e-01 \n",
      "\n",
      "\n",
      "Epoch 14, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.36e-04, IC:3.50e-02, RIC:5.86e-04, prec@10:2.74e-03, SR:6.96e-01\n",
      "Test performance:\n",
      " mse:6.00e-04, IC:3.60e-02, RIC:-1.21e-02, prec@10:1.49e-03, SR:7.91e-01 \n",
      "\n",
      "\n",
      "Epoch 15, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.09e-04, IC:2.91e-02, RIC:-1.83e-03, prec@10:2.22e-03, SR:3.55e-01\n",
      "Test performance:\n",
      " mse:4.32e-04, IC:3.13e-02, RIC:5.13e-04, prec@10:3.08e-03, SR:4.02e-01 \n",
      "\n",
      "\n",
      "Epoch 16, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.91e-04, IC:3.59e-02, RIC:-2.19e-05, prec@10:2.67e-03, SR:7.98e-01\n",
      "Test performance:\n",
      " mse:6.72e-04, IC:3.69e-02, RIC:-7.71e-03, prec@10:1.80e-03, SR:9.06e-01 \n",
      "\n",
      "\n",
      "Epoch 17, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.98e-04, IC:-2.73e-02, RIC:-2.11e-04, prec@10:-1.88e-04, SR:-9.40e-01\n",
      "Test performance:\n",
      " mse:7.85e-04, IC:-2.32e-02, RIC:-1.25e-04, prec@10:1.41e-03, SR:-1.08e+00 \n",
      "\n",
      "\n",
      "Epoch 18, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.59e-04, IC:3.45e-02, RIC:-2.10e-03, prec@10:2.78e-03, SR:5.26e-01\n",
      "Test performance:\n",
      " mse:4.98e-04, IC:3.50e-02, RIC:-3.12e-03, prec@10:3.12e-03, SR:5.92e-01 \n",
      "\n",
      "\n",
      "Epoch 19, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.15e-04, IC:-7.19e-03, RIC:1.86e-03, prec@10:1.00e-03, SR:-3.72e-01\n",
      "Test performance:\n",
      " mse:4.28e-04, IC:-5.95e-03, RIC:-4.99e-03, prec@10:2.51e-03, SR:-4.27e-01 \n",
      "\n",
      "\n",
      "Epoch 20, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.66e-04, IC:2.34e-02, RIC:4.40e-03, prec@10:1.81e-03, SR:2.06e-02\n",
      "Test performance:\n",
      " mse:3.73e-04, IC:2.32e-02, RIC:7.93e-03, prec@10:2.42e-03, SR:2.77e-02 \n",
      "\n",
      "\n",
      "Epoch 21, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.63e-04, IC:3.76e-02, RIC:-1.47e-03, prec@10:3.36e-03, SR:7.51e-01\n",
      "Test performance:\n",
      " mse:6.37e-04, IC:3.78e-02, RIC:5.77e-03, prec@10:2.27e-03, SR:8.53e-01 \n",
      "\n",
      "\n",
      "Epoch 22, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.08e-04, IC:-4.99e-03, RIC:-1.59e-04, prec@10:1.43e-03, SR:-3.48e-01\n",
      "Test performance:\n",
      " mse:4.22e-04, IC:-5.86e-03, RIC:1.23e-04, prec@10:2.35e-03, SR:-4.05e-01 \n",
      "\n",
      "\n",
      "Epoch 23, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.88e-04, IC:3.05e-02, RIC:1.24e-03, prec@10:2.35e-03, SR:2.68e-01\n",
      "Test performance:\n",
      " mse:4.04e-04, IC:3.27e-02, RIC:-1.70e-03, prec@10:2.27e-03, SR:3.00e-01 \n",
      "\n",
      "\n",
      "Epoch 24, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.12e-04, IC:-7.53e-03, RIC:-2.00e-05, prec@10:1.22e-03, SR:-3.63e-01\n",
      "Test performance:\n",
      " mse:4.27e-04, IC:-6.76e-03, RIC:6.66e-03, prec@10:2.46e-03, SR:-4.24e-01 \n",
      "\n",
      "\n",
      "Epoch 25, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.75e-04, IC:2.79e-02, RIC:-8.00e-04, prec@10:2.21e-03, SR:1.78e-01\n",
      "Test performance:\n",
      " mse:3.85e-04, IC:2.88e-02, RIC:5.97e-04, prec@10:1.07e-03, SR:1.93e-01 \n",
      "\n",
      "\n",
      "Epoch 26, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.27e-04, IC:3.48e-02, RIC:-5.15e-04, prec@10:3.37e-03, SR:4.35e-01\n",
      "Test performance:\n",
      " mse:4.55e-04, IC:3.74e-02, RIC:3.67e-03, prec@10:1.77e-03, SR:4.88e-01 \n",
      "\n",
      "\n",
      "Epoch 27, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.79e-04, IC:3.05e-02, RIC:-6.97e-05, prec@10:2.06e-03, SR:2.13e-01\n",
      "Test performance:\n",
      " mse:3.91e-04, IC:3.32e-02, RIC:1.46e-03, prec@10:1.18e-03, SR:2.37e-01 \n",
      "\n",
      "\n",
      "Epoch 28, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.06e-04, IC:3.76e-02, RIC:-8.76e-04, prec@10:2.33e-03, SR:8.28e-01\n",
      "Test performance:\n",
      " mse:6.92e-04, IC:4.00e-02, RIC:-9.40e-03, prec@10:1.76e-03, SR:9.42e-01 \n",
      "\n",
      "\n",
      "Epoch 29, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.46e-04, IC:-1.60e-02, RIC:-1.90e-03, prec@10:8.27e-04, SR:-4.80e-01\n",
      "Test performance:\n",
      " mse:4.67e-04, IC:-9.89e-03, RIC:9.46e-03, prec@10:1.80e-03, SR:-5.56e-01 \n",
      "\n",
      "\n",
      "Epoch 30, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.13e-04, IC:3.55e-02, RIC:-2.74e-03, prec@10:3.58e-03, SR:3.86e-01\n",
      "Test performance:\n",
      " mse:4.39e-04, IC:4.01e-02, RIC:5.11e-04, prec@10:2.18e-03, SR:4.39e-01 \n",
      "\n",
      "\n",
      "Epoch 31, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.87e-04, IC:-5.00e-03, RIC:2.27e-03, prec@10:1.24e-03, SR:-2.48e-01\n",
      "Test performance:\n",
      " mse:3.95e-04, IC:2.09e-03, RIC:-2.77e-03, prec@10:2.51e-03, SR:-2.94e-01 \n",
      "\n",
      "\n",
      "Epoch 32, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.66e-04, IC:2.70e-02, RIC:2.18e-03, prec@10:2.24e-03, SR:9.02e-02\n",
      "Test performance:\n",
      " mse:3.73e-04, IC:3.17e-02, RIC:4.07e-03, prec@10:6.94e-04, SR:9.88e-02 \n",
      "\n",
      "\n",
      "Epoch 33, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.73e-04, IC:7.67e-03, RIC:3.81e-03, prec@10:1.52e-03, SR:-1.55e-01\n",
      "Test performance:\n",
      " mse:3.78e-04, IC:1.20e-02, RIC:-8.96e-04, prec@10:2.14e-03, SR:-1.85e-01 \n",
      "\n",
      "\n",
      "Epoch 34, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.04e-04, IC:-2.33e-02, RIC:-1.86e-03, prec@10:2.13e-04, SR:-6.25e-01\n",
      "Test performance:\n",
      " mse:5.42e-04, IC:-1.62e-02, RIC:-7.86e-04, prec@10:1.50e-03, SR:-7.28e-01 \n",
      "\n",
      "\n",
      "Epoch 35, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.94e-04, IC:-5.39e-03, RIC:2.51e-03, prec@10:1.04e-03, SR:-2.94e-01\n",
      "Test performance:\n",
      " mse:4.03e-04, IC:1.99e-03, RIC:1.96e-02, prec@10:1.61e-03, SR:-3.43e-01 \n",
      "\n",
      "\n",
      "Epoch 36, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.75e-04, IC:3.88e-02, RIC:-2.31e-03, prec@10:3.24e-03, SR:7.70e-01\n",
      "Test performance:\n",
      " mse:6.58e-04, IC:4.09e-02, RIC:7.45e-04, prec@10:1.86e-03, SR:8.85e-01 \n",
      "\n",
      "\n",
      "Epoch 37, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.64e-04, IC:2.54e-02, RIC:-1.57e-03, prec@10:2.15e-03, SR:5.32e-02\n",
      "Test performance:\n",
      " mse:3.70e-04, IC:2.81e-02, RIC:-2.36e-03, prec@10:1.57e-03, SR:4.81e-02 \n",
      "\n",
      "\n",
      "Epoch 38, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.21e-04, IC:-1.64e-02, RIC:-3.92e-03, prec@10:9.06e-04, SR:-3.98e-01\n",
      "Test performance:\n",
      " mse:4.41e-04, IC:-9.97e-03, RIC:-5.76e-03, prec@10:1.63e-03, SR:-4.76e-01 \n",
      "\n",
      "\n",
      "Epoch 39, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.78e-04, IC:3.13e-02, RIC:-1.76e-03, prec@10:2.33e-03, SR:2.21e-01\n",
      "Test performance:\n",
      " mse:3.88e-04, IC:3.67e-02, RIC:-1.10e-02, prec@10:1.63e-03, SR:2.35e-01 \n",
      "\n",
      "\n",
      "Epoch 40, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.63e-04, IC:2.27e-02, RIC:1.48e-03, prec@10:2.22e-03, SR:1.40e-02\n",
      "Test performance:\n",
      " mse:3.68e-04, IC:2.86e-02, RIC:5.24e-03, prec@10:1.27e-03, SR:6.58e-03 \n",
      "\n",
      "\n",
      "Epoch 41, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:7.61e-04, IC:3.90e-02, RIC:1.77e-03, prec@10:1.66e-03, SR:9.14e-01\n",
      "Test performance:\n",
      " mse:7.62e-04, IC:4.07e-02, RIC:8.06e-03, prec@10:1.01e-03, SR:1.04e+00 \n",
      "\n",
      "\n",
      "Epoch 42, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:1.63e-03, IC:3.86e-02, RIC:-1.23e-03, prec@10:1.68e-03, SR:1.64e+00\n",
      "Test performance:\n",
      " mse:1.89e-03, IC:3.84e-02, RIC:2.27e-03, prec@10:-1.03e-03, SR:1.87e+00 \n",
      "\n",
      "\n",
      "Epoch 43, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.75e-04, IC:3.53e-02, RIC:-2.83e-03, prec@10:2.54e-03, SR:1.98e-01\n",
      "Test performance:\n",
      " mse:3.86e-04, IC:3.93e-02, RIC:-2.54e-03, prec@10:1.81e-03, SR:2.17e-01 \n",
      "\n",
      "\n",
      "Epoch 44, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.22e-04, IC:3.86e-02, RIC:-2.20e-03, prec@10:4.07e-03, SR:4.25e-01\n",
      "Test performance:\n",
      " mse:4.50e-04, IC:4.22e-02, RIC:6.50e-03, prec@10:3.16e-03, SR:4.79e-01 \n",
      "\n",
      "\n",
      "Epoch 45, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.68e-04, IC:3.16e-02, RIC:-5.72e-03, prec@10:2.46e-03, SR:1.34e-01\n",
      "Test performance:\n",
      " mse:3.76e-04, IC:3.74e-02, RIC:8.11e-03, prec@10:1.41e-03, SR:1.43e-01 \n",
      "\n",
      "\n",
      "Epoch 46, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.96e-04, IC:-6.16e-03, RIC:-2.49e-03, prec@10:1.14e-03, SR:-3.10e-01\n",
      "Test performance:\n",
      " mse:4.06e-04, IC:1.91e-03, RIC:1.02e-02, prec@10:1.66e-03, SR:-3.62e-01 \n",
      "\n",
      "\n",
      "Epoch 47, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.47e-04, IC:-1.76e-02, RIC:-6.18e-04, prec@10:7.18e-04, SR:-4.86e-01\n",
      "Test performance:\n",
      " mse:4.71e-04, IC:-1.23e-02, RIC:4.35e-03, prec@10:9.97e-04, SR:-5.73e-01 \n",
      "\n",
      "\n",
      "Epoch 48, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.32e-04, IC:3.88e-02, RIC:-1.42e-03, prec@10:3.95e-03, SR:4.59e-01\n",
      "Test performance:\n",
      " mse:4.62e-04, IC:4.29e-02, RIC:-1.76e-03, prec@10:3.92e-03, SR:5.16e-01 \n",
      "\n",
      "\n",
      "Epoch 49, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.63e-04, IC:3.37e-02, RIC:2.75e-03, prec@10:2.42e-03, SR:4.86e-02\n",
      "Test performance:\n",
      " mse:3.69e-04, IC:3.85e-02, RIC:1.09e-03, prec@10:3.15e-03, SR:5.57e-02 \n",
      "\n",
      "\n",
      "Epoch 50, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.53e-04, IC:-3.02e-02, RIC:9.98e-04, prec@10:-1.88e-04, SR:-8.83e-01\n",
      "Test performance:\n",
      " mse:7.30e-04, IC:-2.18e-02, RIC:8.50e-04, prec@10:-6.03e-04, SR:-1.03e+00 \n",
      "\n",
      "\n",
      "Epoch 51, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:8.26e-04, IC:-3.05e-02, RIC:1.72e-03, prec@10:-3.88e-04, SR:-9.84e-01\n",
      "Test performance:\n",
      " mse:8.23e-04, IC:-2.38e-02, RIC:-4.06e-03, prec@10:-9.01e-04, SR:-1.14e+00 \n",
      "\n",
      "\n",
      "Epoch 52, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.27e-04, IC:-1.56e-02, RIC:-9.97e-04, prec@10:8.15e-04, SR:-4.31e-01\n",
      "Test performance:\n",
      " mse:4.45e-04, IC:-7.46e-03, RIC:1.03e-02, prec@10:5.89e-04, SR:-5.04e-01 \n",
      "\n",
      "\n",
      "Epoch 53, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.72e-04, IC:7.99e-03, RIC:1.64e-03, prec@10:1.84e-03, SR:-1.59e-01\n",
      "Test performance:\n",
      " mse:3.78e-04, IC:9.68e-03, RIC:1.39e-02, prec@10:3.26e-03, SR:-2.00e-01 \n",
      "\n",
      "\n",
      "Epoch 54, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.66e-04, IC:1.45e-02, RIC:4.01e-03, prec@10:1.73e-03, SR:-9.23e-02\n",
      "Test performance:\n",
      " mse:3.71e-04, IC:1.72e-02, RIC:5.87e-03, prec@10:3.34e-03, SR:-1.24e-01 \n",
      "\n",
      "\n",
      "Epoch 55, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:1.29e-03, IC:-3.23e-02, RIC:-2.91e-04, prec@10:-2.38e-04, SR:-1.42e+00\n",
      "Test performance:\n",
      " mse:1.42e-03, IC:-2.78e-02, RIC:4.00e-03, prec@10:-1.31e-03, SR:-1.64e+00 \n",
      "\n",
      "\n",
      "Epoch 56, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.62e-04, IC:-1.90e-02, RIC:-4.10e-04, prec@10:8.08e-04, SR:-5.29e-01\n",
      "Test performance:\n",
      " mse:4.91e-04, IC:-1.40e-02, RIC:5.47e-03, prec@10:1.11e-03, SR:-6.23e-01 \n",
      "\n",
      "\n",
      "Epoch 57, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:1.09e-03, IC:-3.12e-02, RIC:4.77e-04, prec@10:-4.87e-05, SR:-1.26e+00\n",
      "Test performance:\n",
      " mse:1.16e-03, IC:-2.65e-02, RIC:-4.28e-03, prec@10:-8.66e-04, SR:-1.46e+00 \n",
      "\n",
      "\n",
      "Epoch 58, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.72e-04, IC:3.61e-02, RIC:-1.73e-03, prec@10:2.51e-03, SR:1.85e-01\n",
      "Test performance:\n",
      " mse:3.81e-04, IC:4.20e-02, RIC:-3.44e-04, prec@10:2.43e-03, SR:1.98e-01 \n",
      "\n",
      "\n",
      "Epoch 59, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.68e-04, IC:3.37e-02, RIC:-5.10e-05, prec@10:2.48e-03, SR:1.49e-01\n",
      "Test performance:\n",
      " mse:3.76e-04, IC:4.33e-02, RIC:-3.49e-03, prec@10:7.54e-04, SR:1.61e-01 \n",
      "\n",
      "\n",
      "Epoch 60, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.66e-04, IC:3.19e-02, RIC:-1.09e-03, prec@10:2.39e-03, SR:1.19e-01\n",
      "Test performance:\n",
      " mse:3.72e-04, IC:4.12e-02, RIC:3.41e-03, prec@10:2.05e-03, SR:1.23e-01 \n",
      "\n",
      "\n",
      "Epoch 61, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.78e-04, IC:3.80e-03, RIC:3.15e-03, prec@10:1.35e-03, SR:-2.18e-01\n",
      "Test performance:\n",
      " mse:3.85e-04, IC:1.11e-02, RIC:2.73e-02, prec@10:3.17e-03, SR:-2.59e-01 \n",
      "\n",
      "\n",
      "Epoch 62, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.77e-04, IC:3.89e-02, RIC:-3.12e-04, prec@10:2.63e-03, SR:2.24e-01\n",
      "Test performance:\n",
      " mse:3.89e-04, IC:4.34e-02, RIC:5.74e-03, prec@10:3.31e-03, SR:2.46e-01 \n",
      "\n",
      "\n",
      "Epoch 63, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.63e-04, IC:3.96e-02, RIC:1.38e-04, prec@10:1.64e-03, SR:9.26e-01\n",
      "Test performance:\n",
      " mse:7.62e-04, IC:4.13e-02, RIC:-2.02e-03, prec@10:1.18e-03, SR:1.05e+00 \n",
      "\n",
      "\n",
      "Epoch 64, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.28e-04, IC:-1.04e-02, RIC:-2.57e-03, prec@10:9.44e-04, SR:-4.39e-01\n",
      "Test performance:\n",
      " mse:4.46e-04, IC:-6.18e-03, RIC:-7.89e-04, prec@10:1.78e-03, SR:-5.10e-01 \n",
      "\n",
      "\n",
      "Epoch 65, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.32e-04, IC:4.19e-02, RIC:-1.65e-03, prec@10:3.26e-03, SR:4.62e-01\n",
      "Test performance:\n",
      " mse:4.62e-04, IC:4.16e-02, RIC:7.40e-03, prec@10:3.29e-03, SR:5.16e-01 \n",
      "\n",
      "\n",
      "Epoch 66, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.93e-04, IC:-2.52e-02, RIC:-1.67e-04, prec@10:4.76e-04, SR:-8.04e-01\n",
      "Test performance:\n",
      " mse:6.52e-04, IC:-2.07e-02, RIC:3.86e-03, prec@10:1.69e-05, SR:-9.30e-01 \n",
      "\n",
      "\n",
      "Epoch 67, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:8.62e-04, IC:3.96e-02, RIC:-1.73e-03, prec@10:1.63e-03, SR:1.05e+00\n",
      "Test performance:\n",
      " mse:8.87e-04, IC:4.08e-02, RIC:4.20e-03, prec@10:7.98e-04, SR:1.19e+00 \n",
      "\n",
      "\n",
      "Epoch 68, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.98e-04, IC:-7.51e-03, RIC:1.99e-03, prec@10:1.18e-03, SR:-3.24e-01\n",
      "Test performance:\n",
      " mse:4.09e-04, IC:9.76e-04, RIC:6.31e-03, prec@10:1.70e-03, SR:-3.81e-01 \n",
      "\n",
      "\n",
      "Epoch 69, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:9.65e-04, IC:3.89e-02, RIC:-7.01e-04, prec@10:1.50e-03, SR:1.17e+00\n",
      "Test performance:\n",
      " mse:1.02e-03, IC:4.06e-02, RIC:-5.98e-05, prec@10:-1.04e-03, SR:1.33e+00 \n",
      "\n",
      "\n",
      "Epoch 70, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.66e-04, IC:1.49e-02, RIC:1.45e-03, prec@10:1.61e-03, SR:-1.14e-01\n",
      "Test performance:\n",
      " mse:3.69e-04, IC:3.14e-02, RIC:-5.61e-03, prec@10:2.85e-03, SR:-1.34e-01 \n",
      "\n",
      "\n",
      "Epoch 71, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.99e-04, IC:3.96e-02, RIC:2.96e-04, prec@10:1.40e-03, SR:9.67e-01\n",
      "Test performance:\n",
      " mse:8.15e-04, IC:4.13e-02, RIC:-7.61e-03, prec@10:7.62e-04, SR:1.10e+00 \n",
      "\n",
      "\n",
      "Epoch 72, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:9.52e-04, IC:3.93e-02, RIC:-3.02e-03, prec@10:1.45e-03, SR:1.15e+00\n",
      "Test performance:\n",
      " mse:1.01e-03, IC:4.05e-02, RIC:1.74e-03, prec@10:-1.01e-03, SR:1.31e+00 \n",
      "\n",
      "\n",
      "Epoch 73, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.28e-04, IC:-1.27e-02, RIC:-3.33e-04, prec@10:8.63e-04, SR:-4.38e-01\n",
      "Test performance:\n",
      " mse:4.46e-04, IC:-7.14e-03, RIC:3.98e-03, prec@10:1.66e-03, SR:-5.11e-01 \n",
      "\n",
      "\n",
      "Epoch 74, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:8.37e-04, IC:-2.92e-02, RIC:-3.89e-05, prec@10:-1.18e-04, SR:-1.01e+00\n",
      "Test performance:\n",
      " mse:8.30e-04, IC:-2.38e-02, RIC:7.04e-03, prec@10:-9.32e-04, SR:-1.17e+00 \n",
      "\n",
      "\n",
      "Epoch 75, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.88e-04, IC:-2.38e-02, RIC:7.44e-04, prec@10:4.85e-04, SR:-5.97e-01\n",
      "Test performance:\n",
      " mse:5.21e-04, IC:-1.58e-02, RIC:-8.34e-03, prec@10:4.83e-04, SR:-6.96e-01 \n",
      "\n",
      "\n",
      "Epoch 76, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:1.01e-03, IC:3.93e-02, RIC:7.31e-04, prec@10:1.41e-03, SR:1.20e+00\n",
      "Test performance:\n",
      " mse:1.09e-03, IC:4.00e-02, RIC:5.45e-03, prec@10:-7.56e-04, SR:1.38e+00 \n",
      "\n",
      "\n",
      "Epoch 77, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.97e-04, IC:-2.20e-02, RIC:-2.78e-04, prec@10:5.49e-04, SR:-6.25e-01\n",
      "Test performance:\n",
      " mse:5.29e-04, IC:-1.47e-02, RIC:-4.64e-03, prec@10:1.08e-03, SR:-7.20e-01 \n",
      "\n",
      "\n",
      "Epoch 78, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.82e-04, IC:6.27e-03, RIC:-2.28e-03, prec@10:9.92e-04, SR:-2.47e-01\n",
      "Test performance:\n",
      " mse:3.88e-04, IC:1.71e-02, RIC:-1.05e-02, prec@10:2.36e-03, SR:-2.85e-01 \n",
      "\n",
      "\n",
      "Epoch 79, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.18e-04, IC:-1.47e-02, RIC:-5.06e-05, prec@10:8.11e-04, SR:-4.00e-01\n",
      "Test performance:\n",
      " mse:4.36e-04, IC:-1.04e-02, RIC:-5.13e-04, prec@10:1.01e-03, SR:-4.77e-01 \n",
      "\n",
      "\n",
      "Epoch 80, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.09e-04, IC:3.99e-02, RIC:-2.89e-03, prec@10:1.40e-03, SR:8.49e-01\n",
      "Test performance:\n",
      " mse:6.86e-04, IC:4.13e-02, RIC:-6.20e-03, prec@10:1.91e-03, SR:9.52e-01 \n",
      "\n",
      "\n",
      "Epoch 81, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:1.05e-03, IC:-3.21e-02, RIC:-8.91e-04, prec@10:-2.41e-04, SR:-1.22e+00\n",
      "Test performance:\n",
      " mse:1.11e-03, IC:-2.78e-02, RIC:-1.74e-03, prec@10:-9.40e-04, SR:-1.42e+00 \n",
      "\n",
      "\n",
      "Epoch 82, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.15e-04, IC:4.10e-02, RIC:-5.62e-04, prec@10:3.14e-03, SR:4.08e-01\n",
      "Test performance:\n",
      " mse:4.37e-04, IC:4.36e-02, RIC:6.26e-03, prec@10:3.53e-03, SR:4.48e-01 \n",
      "\n",
      "\n",
      "Epoch 83, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.67e-04, IC:4.01e-02, RIC:-1.72e-03, prec@10:2.04e-03, SR:1.43e-01\n",
      "Test performance:\n",
      " mse:3.75e-04, IC:4.38e-02, RIC:-3.38e-04, prec@10:3.20e-03, SR:1.54e-01 \n",
      "\n",
      "\n",
      "Epoch 84, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.76e-04, IC:3.13e-03, RIC:-9.59e-04, prec@10:1.33e-03, SR:-2.07e-01\n",
      "Test performance:\n",
      " mse:3.83e-04, IC:9.99e-03, RIC:1.60e-03, prec@10:2.76e-03, SR:-2.51e-01 \n",
      "\n",
      "\n",
      "Epoch 85, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:9.38e-04, IC:-3.15e-02, RIC:9.32e-04, prec@10:-2.94e-04, SR:-1.11e+00\n",
      "Test performance:\n",
      " mse:9.69e-04, IC:-2.72e-02, RIC:6.19e-03, prec@10:-2.91e-04, SR:-1.30e+00 \n",
      "\n",
      "\n",
      "Epoch 86, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.61e-04, IC:2.60e-02, RIC:2.79e-03, prec@10:2.15e-03, SR:-3.42e-02\n",
      "Test performance:\n",
      " mse:3.65e-04, IC:3.66e-02, RIC:-5.96e-04, prec@10:2.49e-03, SR:-5.03e-02 \n",
      "\n",
      "\n",
      "Epoch 87, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.01e-04, IC:3.90e-02, RIC:-2.04e-03, prec@10:1.39e-03, SR:8.30e-01\n",
      "Test performance:\n",
      " mse:6.85e-04, IC:4.18e-02, RIC:1.86e-03, prec@10:-1.79e-04, SR:9.42e-01 \n",
      "\n",
      "\n",
      "Epoch 88, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:1.76e-03, IC:3.85e-02, RIC:1.36e-03, prec@10:1.77e-03, SR:1.72e+00\n",
      "Test performance:\n",
      " mse:2.05e-03, IC:3.79e-02, RIC:5.21e-03, prec@10:-1.18e-03, SR:1.96e+00 \n",
      "\n",
      "\n",
      "Epoch 89, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.63e-04, IC:3.69e-02, RIC:-4.08e-04, prec@10:1.86e-03, SR:8.86e-02\n",
      "Test performance:\n",
      " mse:3.69e-04, IC:4.09e-02, RIC:-2.81e-03, prec@10:2.82e-03, SR:8.59e-02 \n",
      "\n",
      "\n",
      "Epoch 90, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:4.63e-04, IC:1.64e-02, RIC:3.05e-03, prec@10:1.74e-03, SR:-7.66e-02\n",
      "Test performance:\n",
      " mse:3.68e-04, IC:2.38e-02, RIC:7.98e-03, prec@10:2.95e-03, SR:-1.06e-01 \n",
      "\n",
      "\n",
      "Epoch 91, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:6.53e-04, IC:4.07e-02, RIC:3.71e-04, prec@10:1.91e-03, SR:7.50e-01\n",
      "Test performance:\n",
      " mse:6.18e-04, IC:4.08e-02, RIC:-8.87e-03, prec@10:1.59e-03, SR:8.41e-01 \n",
      "\n",
      "\n",
      "Epoch 92, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.17e-04, IC:-1.51e-02, RIC:1.34e-03, prec@10:5.11e-04, SR:-3.99e-01\n",
      "Test performance:\n",
      " mse:4.36e-04, IC:-1.20e-02, RIC:-4.89e-03, prec@10:1.56e-03, SR:-4.77e-01 \n",
      "\n",
      "\n",
      "Epoch 93, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.59e-04, IC:-1.89e-02, RIC:1.74e-03, prec@10:5.11e-04, SR:-5.33e-01\n",
      "Test performance:\n",
      " mse:4.85e-04, IC:-1.45e-02, RIC:1.88e-02, prec@10:2.02e-03, SR:-6.20e-01 \n",
      "\n",
      "\n",
      "Epoch 94, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:5.20e-04, IC:4.10e-02, RIC:1.19e-03, prec@10:2.95e-03, SR:4.31e-01\n",
      "Test performance:\n",
      " mse:4.44e-04, IC:4.37e-02, RIC:3.70e-03, prec@10:3.37e-03, SR:4.72e-01 \n",
      "\n",
      "\n",
      "Epoch 95, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.88e-04, IC:4.04e-02, RIC:9.88e-04, prec@10:3.25e-03, SR:2.94e-01\n",
      "Test performance:\n",
      " mse:4.03e-04, IC:4.60e-02, RIC:1.25e-02, prec@10:3.74e-03, SR:3.23e-01 \n",
      "\n",
      "\n",
      "Epoch 96, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:7.25e-04, IC:-2.84e-02, RIC:-3.94e-03, prec@10:-3.23e-05, SR:-8.47e-01\n",
      "Test performance:\n",
      " mse:7.02e-04, IC:-2.52e-02, RIC:5.20e-03, prec@10:-7.66e-04, SR:-9.93e-01 \n",
      "\n",
      "\n",
      "Epoch 97, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:6.97e-04, IC:-2.65e-02, RIC:-3.42e-03, prec@10:3.04e-05, SR:-8.11e-01\n",
      "Test performance:\n",
      " mse:6.62e-04, IC:-2.38e-02, RIC:-4.52e-03, prec@10:-4.84e-06, SR:-9.45e-01 \n",
      "\n",
      "\n",
      "Epoch 98, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.32e-04, IC:4.12e-02, RIC:-5.91e-04, prec@10:3.75e-03, SR:4.65e-01\n",
      "Test performance:\n",
      " mse:4.61e-04, IC:4.44e-02, RIC:-1.25e-02, prec@10:1.16e-03, SR:5.20e-01 \n",
      "\n",
      "\n",
      "Epoch 99, Training Loss: 0.0005\n",
      "Valid performance:\n",
      " mse:5.03e-04, IC:-1.15e-02, RIC:9.71e-04, prec@10:7.39e-04, SR:-3.48e-01\n",
      "Test performance:\n",
      " mse:4.17e-04, IC:-5.02e-03, RIC:3.44e-03, prec@10:3.16e-03, SR:-4.14e-01 \n",
      "\n",
      "\n",
      "Epoch 100, Training Loss: 0.0006\n",
      "Valid performance:\n",
      " mse:4.65e-04, IC:1.06e-02, RIC:1.76e-03, prec@10:1.16e-03, SR:-9.90e-02\n",
      "Test performance:\n",
      " mse:3.70e-04, IC:2.04e-02, RIC:2.83e-04, prec@10:3.02e-03, SR:-1.34e-01 \n",
      "\n",
      "\n",
      "Best Valid performance:\n",
      " mse:4.61e-04, IC:2.60e-02, RIC:2.79e-03, prec@10:2.15e-03, SR:-3.42e-02\n",
      "Best Test performance:\n",
      " mse:3.65e-04, IC:3.66e-02, RIC:-5.96e-04, prec@10:2.49e-03, SR:-5.03e-02\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Seed initialization\n",
    "np.random.seed(123456789)\n",
    "torch.manual_seed(12345678)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and parameters\n",
    "data_path = '../dataset'\n",
    "market_name = 'NASDAQ'\n",
    "relation_name = 'wikidata'\n",
    "stock_num = 1026\n",
    "lookback_length = 16\n",
    "epochs = 100\n",
    "valid_index = 756\n",
    "test_index = 1008\n",
    "fea_num = 5\n",
    "market_num = 20\n",
    "steps = 1\n",
    "learning_rate = 0.001\n",
    "alpha = 0.1\n",
    "scale_factor = 3\n",
    "\n",
    "dataset_path = os.path.join('../dataset', market_name)\n",
    "if market_name == \"SP500\":\n",
    "    data = np.load(os.path.join(dataset_path, 'SP500.npy'))\n",
    "    data = data[:, 915:, :]\n",
    "    price_data = data[:, :, -1]\n",
    "    mask_data = np.ones((data.shape[0], data.shape[1]))\n",
    "    eod_data = data\n",
    "    gt_data = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for ticket in range(data.shape[0]):\n",
    "        for row in range(1, data.shape[1]):\n",
    "            gt_data[ticket][row] = (data[ticket][row][-1] - data[ticket][row - steps][-1]) / data[ticket][row - steps][-1]\n",
    "else:\n",
    "    with open(os.path.join(dataset_path, '/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/eod_data.pkl'), 'rb') as f:\n",
    "        eod_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, '/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/mask_data.pkl'), 'rb') as f:\n",
    "        mask_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, '/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/gt_data.pkl'), 'rb') as f:\n",
    "        gt_data = pickle.load(f)\n",
    "    with open(os.path.join(dataset_path, '/media/isk/New Volume/Kuliah/Semester_6/Business Intelligence/Code/StockMixer/dataset/NASDAQ/price_data.pkl'), 'rb') as f:\n",
    "        price_data = pickle.load(f)\n",
    "\n",
    "# Define the LinearRegressionModel (add this if you don't already have it)\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Model definition\n",
    "input_dim = lookback_length * fea_num\n",
    "output_dim = 1\n",
    "model = LinearRegressionModel(input_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "batch_offsets = np.arange(0, valid_index, dtype=int)\n",
    "\n",
    "def prepare_input_data(offset, lookback_length, steps):\n",
    "    seq_len = lookback_length\n",
    "    mask_batch = mask_data[:, offset: offset + seq_len + steps]\n",
    "    mask_batch = np.min(mask_batch, axis=1)\n",
    "    return (\n",
    "        eod_data[:, offset:offset + seq_len, :].reshape(-1, input_dim),\n",
    "        np.expand_dims(mask_batch, axis=1),\n",
    "        np.expand_dims(price_data[:, offset + seq_len - 1], axis=1),\n",
    "        np.expand_dims(gt_data[:, offset + seq_len + steps - 1], axis=1)\n",
    "    )\n",
    "\n",
    "def get_batch(offset=None):\n",
    "    if offset is None:\n",
    "        offset = np.random.randint(0, valid_index)\n",
    "    data, mask, price, gt = prepare_input_data(offset, lookback_length, steps)\n",
    "    return torch.tensor(data, dtype=torch.float32).to(device), torch.tensor(mask, dtype=torch.float32).to(device), torch.tensor(price, dtype=torch.float32).to(device), torch.tensor(gt, dtype=torch.float32).to(device)\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def information_coefficient(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    return torch.corrcoef(torch.stack([y_true, y_pred]))[0, 1]\n",
    "\n",
    "def rank_information_coefficient(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    return torch.corrcoef(torch.stack([torch.argsort(y_true), torch.argsort(y_pred)]))[0, 1]\n",
    "\n",
    "def precision_at_k(y_true, y_pred, k=10):\n",
    "    _, topk_indices = torch.topk(y_pred, k, dim=0)\n",
    "    precision = torch.sum(y_true[topk_indices]) / k\n",
    "    return precision\n",
    "\n",
    "def sharpe_ratio(y_true, y_pred):\n",
    "    excess_returns = y_true - y_pred\n",
    "    return torch.mean(excess_returns) / torch.std(excess_returns)\n",
    "\n",
    "def validate(start_index, end_index):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_ic = 0\n",
    "    total_ric = 0\n",
    "    total_prec_10 = 0\n",
    "    total_sharpe5 = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for offset in range(start_index, end_index):\n",
    "            data, mask, price, gt = get_batch(offset)\n",
    "            prediction = model(data)\n",
    "            loss = criterion(prediction * mask, gt * mask)\n",
    "            total_loss += loss.item()\n",
    "            total_ic += information_coefficient(gt * mask, prediction * mask).item()\n",
    "            total_ric += rank_information_coefficient(gt * mask, prediction * mask).item()\n",
    "            total_prec_10 += precision_at_k(gt * mask, prediction * mask, k=10).item()\n",
    "            total_sharpe5 += sharpe_ratio(gt * mask, prediction * mask).item()\n",
    "            count += 1\n",
    "    avg_loss = total_loss / count\n",
    "    avg_ic = total_ic / count\n",
    "    avg_ric = total_ric / count\n",
    "    avg_prec_10 = total_prec_10 / count\n",
    "    avg_sharpe5 = total_sharpe5 / count\n",
    "    return {\n",
    "        'mse': avg_loss,\n",
    "        'IC': avg_ic,\n",
    "        'RIC': avg_ric,\n",
    "        'prec_10': avg_prec_10,\n",
    "        'sharpe5': avg_sharpe5\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "best_valid_perf = None\n",
    "best_test_perf = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    np.random.shuffle(batch_offsets)\n",
    "    train_loss = 0.0\n",
    "    for j in range(valid_index - lookback_length - steps):\n",
    "        data, mask, price, gt = get_batch(batch_offsets[j])\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = criterion(prediction * mask, gt * mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= (valid_index - lookback_length - steps)\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "    val_perf = validate(valid_index, test_index)\n",
    "    print('Valid performance:\\n', 'mse:{:.2e}, IC:{:.2e}, RIC:{:.2e}, prec@10:{:.2e}, SR:{:.2e}'.format(\n",
    "        val_perf['mse'], val_perf['IC'], val_perf['RIC'], val_perf['prec_10'], val_perf['sharpe5']))\n",
    "\n",
    "    test_perf = validate(test_index, len(mask_data))\n",
    "    print('Test performance:\\n', 'mse:{:.2e}, IC:{:.2e}, RIC:{:.2e}, prec@10:{:.2e}, SR:{:.2e}'.format(\n",
    "        test_perf['mse'], test_perf['IC'], test_perf['RIC'], test_perf['prec_10'], test_perf['sharpe5']), '\\n\\n')\n",
    "\n",
    "    if val_perf['mse'] < best_valid_loss:\n",
    "        best_valid_loss = val_perf['mse']\n",
    "        best_valid_perf = val_perf\n",
    "        best_test_perf = test_perf\n",
    "\n",
    "print('Best Valid performance:\\n', 'mse:{:.2e}, IC:{:.2e}, RIC:{:.2e}, prec@10:{:.2e}, SR:{:.2e}'.format(\n",
    "    best_valid_perf['mse'], best_valid_perf['IC'], best_valid_perf['RIC'], best_valid_perf['prec_10'], best_valid_perf['sharpe5']))\n",
    "print('Best Test performance:\\n', 'mse:{:.2e}, IC:{:.2e}, RIC:{:.2e}, prec@10:{:.2e}, SR:{:.2e}'.format(\n",
    "    best_test_perf['mse'], best_test_perf['IC'], best_test_perf['RIC'], best_test_perf['prec_10'], best_test_perf['sharpe5']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
